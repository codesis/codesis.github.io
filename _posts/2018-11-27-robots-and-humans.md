---
layout: post
title:  "Robots and Humans"
date:   2018-11-27 01:00:00 -0600
categories: jekyll update
comments: true
---

*Hello again!*

I've been trying to make disqus work but I've struggled a bit and now I thought - Hey, let's do something which will push me towards the goal of completing this first exam of the course. I won't give up on disqus, but will put it aside for now. 

Now I will answer two questions we have for this exam - about robots and humans. I'm talking about the robots.txt and the humans.txt. What they are, and how I have composed them for my site will soon be answered. Just keep reading, here it comes!

Robots.txt is a page which is mainly read by the Robots. Robots, are programs. These programs traverse the web and can, for example, scan for emails to spam. Most "spam-robots" ignore the robots.txt but other kinds of robots will read it. The robots.txt will tell the robots if they are allowed or not to certain pages of the site. It is up to the server administrator to choose which pages to allow or if certain robots are unallowed for an instance.
So, short-story: the robots.txt are for the robots on the web.

Humans.txt on the other hand are made for humans to read. This page will tell us who built the site. It's not cut in stone what it should contain, but examples are authors, thanks and site. Authors, which helped create the site, thanks to whomever you want to thank (maybe someone who translated the pages?) and site could contain what kinds of programs the site has used and which languages the site has been translated to.

My robots.txt and humans.txt are found at localhost:4000/robots.txt and localhost:4000/humans.txt. I created them by adding two pages to src. As easy as that.